{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate_data(csv_path):\n",
    "    \"\"\"Load CSV data and validate its structure.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('http://localhost:8888/edit/PycharmProjects/Banking_Compliance_V3/banking_compliance_dataset_500_rows.csv')\n",
    "        print(f\"CSV loaded successfully. Shape: {df.shape}\")\n",
    "        print(\"\\nColumn names:\")\n",
    "        print(df.columns.tolist())\n",
    "        print(\"\\nFirst 3 rows:\")\n",
    "        print(df.head(3))\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{csv_path}' not found.\")\n",
    "        print(\"Please ensure the CSV file is in the same directory as the script.\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_embedding(df):\n",
    "    \"\"\"Prepare text data for embedding by combining relevant columns.\"\"\"\n",
    "    # Define text columns that are likely to exist in banking compliance data\n",
    "    potential_text_columns = [\n",
    "        'customer_type', 'full_name_en', 'nationality', 'address_line1',\n",
    "        'city', 'emirate', 'country', 'email_primary', 'kyc_status',\n",
    "        'risk_rating', 'account_type', 'account_subtype', 'account_name',\n",
    "        'account_status', 'dormancy_status', 'exclusion_reason'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only include columns that actually exist in the DataFrame\n",
    "    text_columns = [col for col in potential_text_columns if col in df.columns]\n",
    "    \n",
    "    if not text_columns:\n",
    "        print(\"Warning: None of the expected text columns found in the dataset.\")\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "        # Fallback: use all string columns\n",
    "        text_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        if not text_columns:\n",
    "            print(\"Error: No text columns found for embedding.\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    print(f\"\\nUsing columns for embedding: {text_columns}\")\n",
    "    \n",
    "    # Create a copy to avoid warnings\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle missing values and combine text columns\n",
    "    for col in text_columns:\n",
    "        df_processed[col] = df_processed[col].fillna('').astype(str)\n",
    "    \n",
    "    # Combine relevant text columns into a new column for embedding\n",
    "    df_processed['text_for_embedding'] = df_processed[text_columns].agg(' '.join, axis=1)\n",
    "    \n",
    "    # Clean up redundant spaces and normalize text\n",
    "    df_processed['text_for_embedding'] = (\n",
    "        df_processed['text_for_embedding']\n",
    "        .str.replace(r'\\s+', ' ', regex=True)\n",
    "        .str.strip()\n",
    "        .str.replace('nan', '')  # Remove 'nan' strings that might appear\n",
    "        .str.replace(r'\\s+', ' ', regex=True)  # Clean up again after removing 'nan'\n",
    "        .str.strip()\n",
    "    )\n",
    "    \n",
    "    # Remove empty entries\n",
    "    empty_mask = df_processed['text_for_embedding'].str.len() == 0\n",
    "    if empty_mask.sum() > 0:\n",
    "        print(f\"Warning: {empty_mask.sum()} entries have empty text after processing.\")\n",
    "        df_processed = df_processed[~empty_mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nFinal dataset shape: {df_processed.shape}\")\n",
    "    print(\"\\nSample prepared text (first 3 entries):\")\n",
    "    for i, text in enumerate(df_processed['text_for_embedding'].head(3)):\n",
    "        print(f\"Entry {i+1}: {text[:200]}{'...' if len(text) > 200 else ''}\")\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac72b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_generate_embeddings(df_processed):\n",
    "    \"\"\"Load the BGE model and generate embeddings.\"\"\"\n",
    "    print(\"\\nLoading BGE-large model...\")\n",
    "    try:\n",
    "        # Try the primary model first\n",
    "        model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load BAAI/bge-large-en-v1.5: {e}\")\n",
    "        print(\"Trying fallback model...\")\n",
    "        try:\n",
    "            # Fallback to a smaller but reliable model\n",
    "            model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            print(\"Fallback model loaded successfully.\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to load fallback model: {e2}\")\n",
    "            print(\"Please ensure you have 'sentence-transformers' installed:\")\n",
    "            print(\"pip install sentence-transformers\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    print(\"Generating embeddings for the text data...\")\n",
    "    try:\n",
    "        # Generate embeddings with progress bar\n",
    "        embeddings = model.encode(\n",
    "            df_processed['text_for_embedding'].tolist(),\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32  # Adjust batch size based on your memory\n",
    "        )\n",
    "        \n",
    "        print(f\"Embeddings generated successfully. Shape: {embeddings.shape}\")\n",
    "        return embeddings, model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embeddings: {e}\")\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4447d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_entries(df_processed, embeddings, query_idx=0, top_n=5):\n",
    "    \"\"\"Find the most similar entries to a given query.\"\"\"\n",
    "    if len(embeddings) <= 1:\n",
    "        print(\"Not enough entries in the dataset to calculate similarity.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFinding top {top_n} most similar entries...\")\n",
    "    \n",
    "    # Use the specified index as query\n",
    "    query_embedding = embeddings[query_idx].reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get top N most similar (excluding self if query is from dataset)\n",
    "    if np.isclose(similarities[query_idx], 1.0):\n",
    "        # Exclude self-similarity\n",
    "        similarities[query_idx] = -1  # Set to very low value to exclude from top results\n",
    "    \n",
    "    # Get indices of top similar entries\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    \n",
    "    # Display results\n",
    "    query_text = df_processed.loc[query_idx, 'text_for_embedding']\n",
    "    print(f\"\\nQuery (Entry {query_idx}): {query_text[:200]}{'...' if len(query_text) > 200 else ''}\")\n",
    "    print(f\"\\nTop {top_n} most similar entries:\")\n",
    "    \n",
    "    for rank, idx in enumerate(top_indices):\n",
    "        if similarities[idx] < -0.5:  # Skip if it's the excluded self-similarity\n",
    "            continue\n",
    "            \n",
    "        score = similarities[idx]\n",
    "        \n",
    "        # Get available information for display\n",
    "        display_info = {}\n",
    "        info_columns = ['customer_id', 'full_name_en', 'kyc_status', 'risk_rating', 'account_type']\n",
    "        \n",
    "        for col in info_columns:\n",
    "            if col in df_processed.columns:\n",
    "                display_info[col] = df_processed.loc[idx, col]\n",
    "        \n",
    "        print(f\"\\nRank {rank+1}: Index {idx}, Similarity Score: {score:.4f}\")\n",
    "        for key, value in display_info.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Show a snippet of the text\n",
    "        similar_text = df_processed.loc[idx, 'text_for_embedding']\n",
    "        print(f\"  Text snippet: {similar_text[:150]}{'...' if len(similar_text) > 150 else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to execute the embedding similarity analysis.\"\"\"\n",
    "    csv_path = 'banking_compliance_dataset_500_rows.csv'\n",
    "    \n",
    "    # Step 1: Load and validate data\n",
    "    df = load_and_validate_data(csv_path)\n",
    "    \n",
    "    # Step 2: Prepare text for embedding\n",
    "    df_processed = prepare_text_for_embedding(df)\n",
    "    \n",
    "    # Step 3: Generate embeddings\n",
    "    embeddings, model = load_model_and_generate_embeddings(df_processed)\n",
    "    \n",
    "    # Step 4: Find similar entries\n",
    "    find_similar_entries(df_processed, embeddings, query_idx=0, top_n=5)\n",
    "    \n",
    "    # Optional: Save embeddings for future use\n",
    "    save_embeddings = input(\"\\nWould you like to save the embeddings to a file? (y/n): \").lower().strip()\n",
    "    if save_embeddings == 'y':\n",
    "        try:\n",
    "            # Save DataFrame with embeddings\n",
    "            df_processed['embedding'] = list(embeddings)\n",
    "            df_processed.to_pickle('banking_compliance_with_embeddings.pkl')\n",
    "            print(\"Embeddings saved to 'banking_compliance_with_embeddings.pkl'\")\n",
    "            \n",
    "            # Also save just the embeddings as numpy array\n",
    "            np.save('banking_compliance_embeddings.npy', embeddings)\n",
    "            print(\"Raw embeddings saved to 'banking_compliance_embeddings.npy'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving embeddings: {e}\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab9572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
